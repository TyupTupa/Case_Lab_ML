{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0342e6d-352e-45a5-aba1-03047e5aafea",
   "metadata": {},
   "source": [
    "# Обучение модели\n",
    "на основе представленных данных из архива acllmdb.zip\n",
    "\n",
    "Процесс работы с данными состоит из этапов:\n",
    "1. Импорт необходимых модулей и библиотек для предобработки;\n",
    "2. Создание датафреймов;\n",
    "3. Предобработка (очистка и подготовка целевой переменной);\n",
    "4. Импорт библиотеки для обучения модели;\n",
    "5. Обучение модели и проверка точности предсказаний;\n",
    "6. Тестирование;\n",
    "7. Экспорт модели.\n",
    "\n",
    "Выбор модели классификации представлен в файле 'Отчет_Case_Lab_ML_Клинова_Мария.docx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346d582-e5a9-4218-a778-1feafe03cf17",
   "metadata": {},
   "source": [
    "## Импорт необходимых модулей и библиотек для предобработки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0a6896-d569-4c83-8b5e-8336ceeaade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b42175a-8c79-40ec-8a5a-fe1dd03a9d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\masha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\masha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\masha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\masha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ff5cf8-14b7-4afc-b759-4f86fd260088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isn', 'mightn', \"don't\", 'does', 'ain', 'before', 'up', \"mightn't\", 'your', 'this', 'are', \"weren't\", 'he', 'him', 'needn', \"shouldn't\", 'not', 'm', 'herself', 'further', 'having', 'too', 'for', 'my', 'an', 'by', 'there', 'when', 'me', 'once', 'all', 'hadn', 'will', 'it', \"aren't\", 'after', 'were', 'ma', 'other', \"should've\", 'shan', 'with', 't', 'under', 'i', 'nor', 'wouldn', 'from', 'about', 'our', 'yourselves', 'which', 'in', 'both', 'weren', 'mustn', \"wasn't\", \"didn't\", \"won't\", 'yours', 'ours', 'again', 'same', 'himself', 'where', 'we', 'at', 'they', 'yourself', 'on', 'only', 'don', 'any', 'd', 'theirs', 'a', 'haven', 'between', 'own', 'wasn', 'has', \"mustn't\", 'll', 'over', 'no', \"you're\", \"doesn't\", 'be', 'hasn', 'won', 'hers', 'o', 'against', 'aren', \"she's\", 'just', 'most', 'how', 's', \"needn't\", 'themselves', 'as', 'until', \"wouldn't\", \"you've\", 'if', 've', \"shan't\", 'been', 'y', 'or', \"that'll\", \"isn't\", 'couldn', 're', 'to', 'its', 'than', 'didn', 'through', 'into', \"it's\", 'who', 'why', 'being', \"hasn't\", 'above', 'shouldn', 'few', 'very', 'then', 'his', 'now', 'you', \"hadn't\", 'myself', 'their', 'had', 'her', 'more', 'whom', 'those', 'below', 'itself', 'ourselves', 'have', 'here', 'what', 'that', 'out', 'and', 'some', 'the', 'while', 'such', 'them', 'did', 'can', 'so', 'is', 'do', 'because', 'of', 'but', 'during', \"haven't\", 'doing', 'off', 'am', \"you'll\", 'down', 'each', 'should', \"you'd\", 'these', \"couldn't\", 'she', 'doesn', 'was'}\n"
     ]
    }
   ],
   "source": [
    "#импорт стоп-слов английского языка\n",
    "en_stops = set(stopwords.words('english')) \n",
    "print(en_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624548c0-5b67-4ace-a101-23ea62ff94d1",
   "metadata": {},
   "source": [
    "## Создание датафреймов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f1864d-db9e-43f8-b5a8-cd3b8d3c69da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labeledBow.feat', 'neg', 'pos', 'unsup', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "print(os.listdir('aclImdb/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6aed68-7cc5-4f51-99e3-73726b45bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_df(type):\n",
    "    \"\"\"\n",
    "    Создает датафрейм с колонками: 'name' (название файла), \n",
    "        'text' (текст файла) и 'sentiment' (окраска текста).\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    def extract_rate(file_name):\n",
    "        return int(file_name.split('_')[1].split('.')[0])\n",
    "        \n",
    "    for name in os.listdir(f'aclImdb/{type}/neg'):\n",
    "        with open(f'aclImdb/{type}/neg/{name}', 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            rate = extract_rate(name)\n",
    "            new_row = pd.DataFrame({'name': [name], 'text': [text], 'sentiment': ['negative'], 'rate': [rate]})\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "    for name in os.listdir(f'aclImdb/{type}/pos'):\n",
    "        with open(f'aclImdb/{type}/pos/{name}', 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            rate = extract_rate(name)\n",
    "            new_row = pd.DataFrame({'name': [name], 'text': [text], 'sentiment': ['positive'], 'rate': [rate]})\n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  #перемешивает строки датафрейма\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8540859-80f7-48d3-8bc6-5a9eda8e89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = creating_df('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff1982b-6d1b-4928-95a7-0dadaee6e85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name                                               text  \\\n",
      "0       4847_1.txt  As usual, I am making a mad dash to see the mo...   \n",
      "1      10747_4.txt  Camp Blood III is a vast improvement on Camp B...   \n",
      "2       6766_3.txt  You probably heard this phrase when it come to...   \n",
      "3       6565_2.txt  This movie is chilling reminder of Bollywood b...   \n",
      "4       7306_3.txt  Truly one of the most dire films I've ever sat...   \n",
      "...            ...                                                ...   \n",
      "24995  12047_1.txt  I had the greatest enthusiasm going in to the ...   \n",
      "24996   3662_9.txt  This is a really strange film--and that is NOT...   \n",
      "24997  1395_10.txt  This one's a romp; many Trek fans don't rate t...   \n",
      "24998   5351_7.txt  Although this film is somewhat filled with eig...   \n",
      "24999   2388_3.txt  No one should ever try to adapt a Tom Robbins ...   \n",
      "\n",
      "      sentiment  rate  \n",
      "0      negative     1  \n",
      "1      negative     4  \n",
      "2      negative     3  \n",
      "3      negative     2  \n",
      "4      negative     3  \n",
      "...         ...   ...  \n",
      "24995  negative     1  \n",
      "24996  positive     9  \n",
      "24997  positive    10  \n",
      "24998  positive     7  \n",
      "24999  negative     3  \n",
      "\n",
      "[25000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cc236-5d3f-41e4-b521-28d28b29b909",
   "metadata": {},
   "source": [
    "## Предобработка"
   ]
  },
  {
   "cell_type": "raw",
   "id": "263a6887-a8e5-4f01-b6ba-a831391adb17",
   "metadata": {},
   "source": [
    "Просмотрим набор уникальных слов из первых 200 строк датафрейма, собранного для обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05f1a570-dc31-4f4c-953f-f2b7d4692d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!\"', '\"', '\"1\"', '\"24\"', '\"A', '\"AND', '\"According', '\"Ah,', '\"Air', '\"Andrew\"', '\"Apart', '\"Baise', '\"Bevan\",', '\"Blair', '\"Bloodrayne', '\"Bloodsuckers\"', '\"Bottom', '\"Buh-bye,', '\"Bullfighter\"', '\"Catwoman\"', '\"Chad\",', '\"China', '\"Close', '\"Cradle', '\"Cursed\"', '\"Damian\\'s', '\"Davitelj', '\"Dead', '\"Deadly', '\"Deerhunter\"', '\"Deewar\"', '\"Devil', '\"Didn\\'t', '\"Dog', '\"Dragnet\"', '\"Dude,', '\"E.T.\"', '\"E.T.:', '\"Eh?', '\"Evil', '\"Extension', '\"Flightplan\"', '\"Frenchfilm\"', '\"From', '\"Full', '\"Fulltime', '\"Gadsden', '\"Golden', '\"Golly', '\"Hammer\"', '\"Herbie:', '\"Hey', '\"How', '\"I', '\"If', '\"In', '\"Invasion', '\"Jenifer\".', '\"Kako', '\"Last', '\"Leatherfaces\".', '\"Leonard\"', '\"Leonora,', '\"Lies\"', '\"Little', '\"MacGyver\"', '\"Maratonci', '\"My', '\"Naked', '\"National', '\"New', '\"No,', '\"OMG', '\"On', '\"Osama', '\"Party', '\"Pilot\"', '\"Plan', '\"Platoon\".', '\"Poet\"', '\"Projected', '\"Pulp', '\"Puppet', '\"Putz\"', '\"Raiders', '\"Revenge', '\"Roll\\'em\"', '\"Rondo\"', '\"Sabretooth\"', '\"Samehada', '\"Saving', '\"Sayonara\"', '\"Schindler\\'s', '\"Semana', '\"September', '\"Shark', '\"She', '\"Shoot', '\"Slither\",', '\"Sloggy\".', '\"Spider', '\"Star', '\"Suspiria\"', '\"Tape', '\"Teen', '\"Texasville\"', '\"The', '\"They', '\"Twin', '\"Vampires', '\"Vampires\")', '\"Voorhess\"', '\"W\"', '\"Whatever\".This', '\"X-Files\"-', '\"Yankee', '\"Yes,', '\"You', '\"Zabriskie', '\"a', '\"artsy\".', '\"audience\"', '\"awesomely', '\"banjo', '\"cheese\"', '\"commitment\".<br', '\"commitophobe\"', '\"creature\\'s', '\"decent\"', '\"demons\"', '\"descent', '\"disappeared\"', '\"document\"', '\"documentary', '\"documentary.\")<br', '\"done', '\"everyday', '\"exterminating\"', '\"fairy', '\"festival\"', '\"film', '\"filthy\"', '\"for', '\"foreign', '\"genetic', '\"good', '\"have', '\"healed\"', '\"heroic\"', '\"highlights\"', '\"homely\"', '\"how', '\"humorous\"', '\"in', '\"inferno\"', '\"inside\"', '\"is\"', '\"it', '\"its', '\"knuckle-busting\"', '\"lab', '\"locked', '\"look', '\"loving-war-like-a-drug\"?..', '\"made\"', '\"magic\"', '\"master\".', '\"master-slave\"-couple', '\"military', '\"movie\"', '\"mystery', '\"near', '\"oh,', '\"okay\"', '\"only\"', '\"party', '\"perverse\"', '\"princesses\"', '\"professionals\"', '\"random-clips-of-obviously-fake-and-tacky-violence-and-an-ugly-', '\"rapture\"', '\"real\")', '\"right', '\"rosebud\"', '\"say', '\"screw', '\"sentimental', '\"serious\"', '\"shaky', '\"sky', '\"smalls\".', '\"space\"', '\"start\"', '\"stupid\"', '\"talent\"?', '\"talking\"', '\"thank', '\"the', '\"thirty', '\"this', '\"tragicomedy\"', '\"trasforms\"', '\"ugly', '\"umm,', '\"very', '\"what', '\"world', '\"worthy', '\"wow', '\"you', '#an', '$.50', '$1,000,000.', '$12.', '$15,000,000', '$3).', '$3.99', '$5', '&', '&,', \"'68.\", \"'70s\", \"'About\", \"'B'\", \"'Being\", \"'Che'\", \"'Crashers',\", \"'Crashers'.\", \"'Crossroads'\", \"'Descendant'\", \"'Frankie\", \"'Hamlet'\", \"'Hamlet,'\", \"'Henry\", \"'In\", \"'Los\", \"'Maiden\", \"'March\", \"'Morpheus',\", \"'Oh\", \"'Prisoner\", \"'Rashomon'\", \"'Royale,\", \"'Tadpole'\", \"'The\", \"'Wonderland'\", \"'actors'\", \"'bad'\", \"'classic'\", \"'distinct\", \"'earth\", \"'em\", \"'entertaining'\", \"'foppish\", \"'forbidden'\", \"'funny'\", \"'gear'\", \"'idea\", \"'idea'.\", \"'killer'\", \"'modern'\", \"'murder'\", \"'never\", \"'not\", \"'occult\", \"'oh\", \"'remaindered'\", \"'special\", \"'story'\", \"'synth'\", \"'thing'\", \"'til\", \"'universality\", \"'waxing'\", \"'where\", '(!!!)', '(!)', '(\"Shark', '(\"They', '(\"democracy', '(&', '(1', '(17', '(1942)', '(1961),', '(1965;', '(1968)', '(1973)!', '(1981)', '(2', \"(Al's\", '(Always', '(Annie', '(Aribert', '(Barbara', '(C-)', '(Carol', '(Charles', '(Charlie', '(Chris', '(Christina', '(Christopher', '(Come', '(Commando,', '(Courtesy', '(Dario', '(David', '(Deputy', '(Diaz', '(Dylan', '(Eric', '(Frankie', '(Goldberg', '(Grade:', '(Guerilla)', '(Guerilla).', '(Hedy', '(Hongmei', '(I', \"(I'm\", '(I)', '(Jane', '(Janeane', '(Jon', '(Jose', '(Josh', '(Just', '(Kate', '(Kato', '(Ladislav', '(Lisa', '(Ludmila', '(Marsha', '(Maybe', '(Merry', '(Michel', '(Michelle', '(NOT', '(Natasha', '(Note', '(Oliver', '(Or', '(Parents', '(Paris', '(RENT,', '(Reb', '(Shelly)', \"(Sijan's\", '(Sybil', '(THE', '(Ted', '(The', '(This', '(Tim', '(Tom', '(Tony', '(Val', '(WHERE', '(Wei', '(Which', '(World', '(Yu', '(Zvonimir', '(`Hey', '(a', '(accident', '(after', '(again,', '(albeit', '(along', '(also', '(although', '(among', '(amputees', '(and', '(and,', '(appearing', '(as', '(at', '(beautiful', '(borrowed', '(both', '(but', '(by', '(check', '(complete', '(dare', \"(don't\", '(during', '(e.g.', '(either', '(especially', '(especialy', '(even', '(explitive)', '(featuring', '(for', '(forget', '(frightfully', '(from']\n"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(train['text'][:200])\n",
    "unique_words = sorted(set(all_text.split()))\n",
    "print(unique_words[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe818f-5c0f-4a3f-b5b2-81cf9b4d11b6",
   "metadata": {},
   "source": [
    "Список содержит разнообразные слова: от слов, которые могут нести определенный смылсл, до слов, которые представляют собой набор букв и цифр. Так же присуствуют знаки препинания и прочие символы, которые не несут полезной информации для анализа. Поэтому такой текст необходмо очистить для повышения точности пресказаний модели, которая будет на них обучаться."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1343739-1aec-4da9-9505-d01a33d2e48d",
   "metadata": {},
   "source": [
    "**Очистка текстов отзывов** состоит из следующих шагов:\n",
    "1. Приведение к нижнему регистру;\n",
    "2. Удаление тегов <\\/br>, найденных в тексте;\n",
    "3. Удаление всех вспомогательных символов и пунктуации;\n",
    "4. Замена всех чисел или слов, содержащих числа (например '50s'), на 'digit';\n",
    "5. Удаление коротких слов из 1-2 букв;\n",
    "6. Токенизация текста;\n",
    "7. Лемматизация токенов;\n",
    "8. Удаление стоп-слов;\n",
    "9. Объединение слов обратно в строку;\n",
    "10. Перемешивание строк датафрейма.\n",
    "\n",
    "Целевая переменняая sentiment_bin необходима для сравнения методов обучения модели, выбора способа классификации и предсказания рейтинга фильма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc6383fe-c2fc-4494-a3a1-0cb732e2b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(df):\n",
    "    \"\"\"\n",
    "    Очищает текстовые данные в датафрейме\n",
    "    \"\"\"\n",
    "    \n",
    "    df['text'] = df['text'].str.lower() \n",
    "    df['text'] = df['text'].str.replace('</br>', '', regex=False)\n",
    "    df['text'] = df['text'].str.replace(r'[^a-zA-Zа-я0-9\\s]', '', regex=True)\n",
    "    df['text'] = df['text'].str.replace(r'\\b\\d+\\b|\\b\\d+\\w*', 'digit', regex=True)\n",
    "    df['text'] = df['text'].str.replace(r'\\b\\w{1,2}\\b', '', regex=True)\n",
    "    df['text'] = df['text'].apply(word_tokenize)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    df['text'] = df['text'].apply(lambda words: [lemmatizer.lemmatize(word) for word in words])\n",
    "    df['text'] = df['text'].apply(lambda words: [word for word in words if word not in en_stops])\n",
    "    df['text'] = df['text'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "    df['sentiment_bin'] = df['sentiment'].map({'positive': 1, 'negative': 0}) #создание новой целевой переменной\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc32a93a-5f14-4cf1-a0b5-31ca48054928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name                                               text  \\\n",
      "0       4847_1.txt  usual making mad dash see movie havent watched...   \n",
      "1      10747_4.txt  camp blood iii vast improvement camp blood ha ...   \n",
      "2       6766_3.txt  probably heard phrase come movie herbie fully ...   \n",
      "3       6565_2.txt  movie chilling reminder bollywood parasite hol...   \n",
      "4       7306_3.txt  truly one dire film ive ever sat ive never act...   \n",
      "...            ...                                                ...   \n",
      "24995  12047_1.txt  greatest enthusiasm going advance screening mo...   \n",
      "24996   3662_9.txt  really strange filmand bad thing combination n...   \n",
      "24997  1395_10.txt  one romp many trek fan dont rate high wellknow...   \n",
      "24998   5351_7.txt  although film somewhat filled eighty cheese pl...   \n",
      "24999   2388_3.txt  one ever try adapt tom robbins book screen mov...   \n",
      "\n",
      "      sentiment  rate  sentiment_bin  \n",
      "0      negative     1              0  \n",
      "1      negative     4              0  \n",
      "2      negative     3              0  \n",
      "3      negative     2              0  \n",
      "4      negative     3              0  \n",
      "...         ...   ...            ...  \n",
      "24995  negative     1              0  \n",
      "24996  positive     9              1  \n",
      "24997  positive    10              1  \n",
      "24998  positive     7              1  \n",
      "24999  negative     3              0  \n",
      "\n",
      "[25000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "train = text_cleaner(train)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96b2a19-cb90-4dc1-82af-a38d0246adce",
   "metadata": {},
   "source": [
    "Заново просмотрим набор уникальных слов из первых 200 строк датафрейма. Теперь данные очищены, это повысит точность модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11ea9c49-7019-42f7-869c-59a56154b875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandoned', 'abc', 'abducted', 'ability', 'abilitytalent', 'able', 'aboard', 'abomination', 'abort', 'aboutbr', 'aboveground', 'abovementioned', 'abovethelaw', 'abraham', 'absolute', 'absolutely', 'absurd', 'absurdity', 'abusing', 'abusive', 'academy', 'accelerating', 'accent', 'acceptable', 'acceptance', 'accepted', 'access', 'accessible', 'accident', 'accidental', 'accidentally', 'acclaim', 'acclaimed', 'accompanied', 'accomplishes', 'accomplishment', 'according', 'account', 'accurately', 'accused', 'ace', 'acharya', 'achieve', 'achievement', 'achieving', 'achingly', 'acknowledged', 'across', 'act', 'acted', 'acting', 'actingbr', 'action', 'actionsbr', 'activity', 'actor', 'actoractress', 'actorwriterdirectors', 'actress', 'actual', 'actually', 'ad', 'adam', 'adapt', 'adaptation', 'adaption', 'add', 'added', 'addictbr', 'adding', 'additional', 'additionally', 'address', 'adequate', 'adjusted', 'administer', 'administration', 'administrative', 'administrator', 'admirably', 'admire', 'admit', 'admitted', 'adolph', 'adrian', 'adult', 'adultery', 'advance', 'advantage', 'adventure', 'adventurebr', 'adventurer', 'advert', 'advice', 'advise', 'aerial', 'affair', 'affectation', 'affected', 'affectedly', 'affinity', 'affront', 'afghanistan', 'aflame', 'aforementioned', 'afraid', 'africa', 'african', 'age', 'aged', 'ageing', 'agent', 'agentine', 'aggression', 'aggrieved', 'ago', 'agony', 'agree', 'agreed', 'ahead', 'aide', 'aimlessly', 'aint', 'air', 'aircraft', 'aired', 'airplane', 'airwolf', 'airwolfs', 'akshay', 'al', 'ala', 'alamo', 'alan', 'alarm', 'albeit', 'albert', 'alcoholic', 'aleination', 'alevels', 'alexandra', 'ali', 'alice', 'alida', 'alien', 'alienated', 'alienation', 'alike', 'alikebr', 'alive', 'allbr', 'allen', 'alley', 'alliesbr', 'allowed', 'allowing', 'allows', 'alltime', 'allude', 'alluring', 'almighty', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'altogether', 'always', 'amateur', 'amateurish', 'amateurishbr', 'amazed', 'amazes', 'amazing', 'amazingly', 'ambitious', 'ambush', 'ameche', 'ameches', 'america', 'american', 'ames', 'amick', 'among', 'amongst', 'amount', 'amounted', 'amounting', 'ample', 'amputee', 'amuse', 'amusement', 'amusing', 'analysis', 'analyzed', 'anastasia', 'anchor', 'anchorman', 'ancient', 'anddoes', 'anderson', 'andie', 'andor', 'andre', 'andrew', 'android', 'andthekilleris', 'andy', 'andys', 'angel', 'angeles', 'angerbr', 'angle', 'angry', 'anil', 'animal', 'animated', 'animation', 'anime', 'ann', 'anna', 'anne', 'annie', 'announces', 'annoyance', 'annoying', 'annoyingbr', 'anns', 'anorectic', 'another', 'answer', 'anthology', 'antianxiety', 'anticipating', 'anticipation', 'anticlassic', 'anticlimatic', 'antidepression', 'antipsychotic', 'antithesis', 'anton', 'antonio', 'antonioni', 'antonionis', 'anybody', 'anymore', 'anyone', 'anyones', 'anything', 'anytime', 'anyway', 'anywaybr', 'anywhere', 'apart', 'apartment', 'apatow', 'ape', 'apollo', 'apology', 'appalling', 'appallingly', 'apparantly', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appeared', 'appearing', 'appears', 'appelation', 'appendix', 'applause', 'applegate', 'appliance', 'apply', 'appreciate', 'appreciated', 'appreciation', 'appreciative', 'approach', 'approachbr', 'approve', 'approximately', 'aptly', 'aquarius', 'arbitrarily', 'arcane', 'archaic', 'architecture', 'archive', 'arent', 'argentine', 'argentinian', 'argento', 'argentos', 'argue', 'argument', 'aribert', 'arie', 'arizona', 'ark', 'arkin', 'arm', 'armstrong', 'armwaving', 'army', 'arnie', 'around', 'aroundbut', 'arouses', 'array', 'arrival', 'arrive', 'arrives', 'arrogance', 'arrogant', 'arrow', 'arsenal', 'art', 'arthouse', 'arthur', 'arthurbr', 'article', 'artist', 'artistic', 'artistwritergenius', 'artisty', 'artsy', 'asano', 'asapbr', 'ascertain', 'ashamed', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'asleep', 'asleepit', 'aspect', 'assassinated', 'assault', 'asset', 'assigned', 'assist', 'assistant', 'associate', 'associated', 'assume', 'assumed', 'assuming', 'astaire', 'asteroid', 'asthma', 'astin', 'astonishing', 'astound', 'astounding', 'astute', 'atilla', 'atmosphere', 'atrocious', 'attached', 'attack', 'attacked', 'attempt', 'attempted', 'attempting', 'attending', 'attends', 'attention', 'attitude', 'attract', 'attraction', 'attractive', 'audience', 'audio', 'august', 'aunt', 'aussie', 'austin', 'australian', 'authenic', 'authentic', 'author', 'automatic', 'automatically', 'ava', 'available', 'avenge', 'avenue', 'average', 'averagebr', 'avert', 'avery']\n"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(train['text'][:200])\n",
    "unique_words = sorted(set(all_text.split()))\n",
    "print(unique_words[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1d273f3-c8b4-4667-b9c3-d2f580b78e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('cleaned_train_data.csv') #сохранение очищенных данных в .csv файл "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587da1cb-15e0-445a-a5cb-c87a77a7d626",
   "metadata": {},
   "source": [
    "Аналогичные действия с данными для тестирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcb371f9-fdfc-4128-b75b-c00075ac8046",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = creating_df('test')\n",
    "test = text_cleaner(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40ae59d8-d02a-4d4c-81d0-f18ad21cdd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              name                                               text  \\\n",
      "0       4989_2.txt  let start saying never wanted see movie first ...   \n",
      "1        678_1.txt  wa wondering possessed organizer victoria film...   \n",
      "2      11258_8.txt  wasnt planning watching wasted saw mtv preview...   \n",
      "3       405_10.txt  note writing review see listing indeed series ...   \n",
      "4       2074_1.txt  like guy said sux count word said entire movie...   \n",
      "...            ...                                                ...   \n",
      "24995   1272_8.txt  great british director christopher nolan momen...   \n",
      "24996   5165_2.txt  wonderful image good intention come yet anothe...   \n",
      "24997   5810_1.txt  hard know exactly say ever bland dull little f...   \n",
      "24998   2733_3.txt  terry cunningham directs scifi network origina...   \n",
      "24999  6724_10.txt  saw korean version daisy first came across sim...   \n",
      "\n",
      "      sentiment  rate  sentiment_bin  \n",
      "0      negative     2              0  \n",
      "1      negative     1              0  \n",
      "2      positive     8              1  \n",
      "3      positive    10              1  \n",
      "4      negative     1              0  \n",
      "...         ...   ...            ...  \n",
      "24995  positive     8              1  \n",
      "24996  negative     2              0  \n",
      "24997  negative     1              0  \n",
      "24998  negative     3              0  \n",
      "24999  positive    10              1  \n",
      "\n",
      "[25000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a072edcc-2373-4a1a-9fab-30796ee7dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('cleaned_test_data.csv') #сохранение очищенных данных в .csv файл "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e7cea-c641-45a4-b32b-fda4d432249f",
   "metadata": {},
   "source": [
    "## Импорт библиотек для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7dd247d-cdc1-4900-88e7-509b0e847df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ab99821-6339-491b-bd0e-1327194ef75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f80d053b-b43d-4077-8abc-7b4958f56325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dfa2b0-bdef-4d6f-855f-40464185fc63",
   "metadata": {},
   "source": [
    "## Обучение моделей и проверка точности предсказаний"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7368492d-5ebe-4e06-9ced-35695561b275",
   "metadata": {},
   "source": [
    "Для предсказания рейтинга фильма рассмотрим **два способа**:\n",
    "1. Мультиклассовая классификация с целью предсказания рейтинга фильма, целевые значения - train['rate'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51e17a56-789d-4bd7-90b4-6d88c93fdf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пайплайн последовательно векторизует числа, после этого применяет алгорим классификации\n",
    "pipe_text_multiclass_classfication = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1, 3), max_features=20000)),\n",
    "    ('pca', PCA(n_components=1000)),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state = 42))\n",
    "])\n",
    "\n",
    "#разделение данных\n",
    "features_train, features_test, target_train, target_test = train['text'], test['text'], train['rate'], test['rate']\n",
    "\n",
    "pipe_text_multiclass_classfication.fit(features_train, target_train)\n",
    "prediction = pipe_text_multiclass_classfication.predict(features_test)\n",
    "\n",
    "#проверка точности\n",
    "accuracy_multiclass = accuracy_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab2e36fb-8d79-420f-b89b-b76ac3ea13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42272\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_multiclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffc34eb-2774-4ca9-a71f-5c77bbce4c07",
   "metadata": {},
   "source": [
    "2. Бинарная классификация отзывов на положительные и негативные, рейтинг фильма определяется как вероятность принадлежности к классу \"positive\", с округление в большую сторону, целевые значения - train['sentiment_bin']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "136dac8b-a46a-4c4a-992a-99d751f66a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#пайплайн последовательно векторизует числа, после этого применяет алгорим классификации\n",
    "pipe_text_binary_classification = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1, 3), max_features=10000)), \n",
    "    ('classifier', LogisticRegression(max_iter=5000))\n",
    "])\n",
    "\n",
    "#разделение данных\n",
    "features_train, features_test, target_train, target_test = train['text'], test['text'], train['sentiment_bin'], test['sentiment_bin']\n",
    "\n",
    "pipe_text_binary_classification.fit(features_train, target_train)\n",
    "prediction = pipe_text_binary_classification.predict(features_test)\n",
    "\n",
    "#проверка точности\n",
    "accuracy_binary = accuracy_score(target_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "775a534a-d0a6-4355-8e27-56a5c325d072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88472\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e613d9-05e6-4365-b74d-e9f190eafcb4",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa0269-ee9b-4982-a42b-184e2b44b25e",
   "metadata": {},
   "source": [
    "На основе датафрейма test, предскажем рейтинг фильма мультиклассовой моделью, окраску текста бинарной моделью и рейтинг на осонове вероятности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5ef8269-2737-4f7e-a269-5df110ecae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predicted_rate_multiclass'] = pipe_text_multiclass_classfication.predict(test['text'])\n",
    "test['predicted_semtiment_binary'] = pipe_text_binary_classification.predict(test['text'])\n",
    "test['probability_rate'] = np.ceil(pipe_text_binary_classification.predict_proba(test['text'])[:, 1] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b604c133-f4c4-4d6d-b86f-31fb269d00f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          name                                               text sentiment  \\\n",
      "0   4989_2.txt  let start saying never wanted see movie first ...  negative   \n",
      "1    678_1.txt  wa wondering possessed organizer victoria film...  negative   \n",
      "2  11258_8.txt  wasnt planning watching wasted saw mtv preview...  positive   \n",
      "3   405_10.txt  note writing review see listing indeed series ...  positive   \n",
      "4   2074_1.txt  like guy said sux count word said entire movie...  negative   \n",
      "5  4404_10.txt  thank goodness coen brother success ha brought...  positive   \n",
      "6      5_7.txt  went see movie always liked kevin costner felt...  positive   \n",
      "7   4995_1.txt  cant give le star tried moment sure halfway st...  negative   \n",
      "8  5789_10.txt  always knew day wa coming knew much oil ground...  positive   \n",
      "9   9971_9.txt  saw toronto international film festival beauti...  positive   \n",
      "\n",
      "   rate  sentiment_bin  predicted_rate_multiclass  predicted_semtiment_binary  \\\n",
      "0     2              0                          1                           0   \n",
      "1     1              0                          1                           0   \n",
      "2     8              1                          3                           0   \n",
      "3    10              1                          7                           1   \n",
      "4     1              0                          1                           0   \n",
      "5    10              1                         10                           1   \n",
      "6     7              1                          8                           1   \n",
      "7     1              0                          1                           0   \n",
      "8    10              1                         10                           1   \n",
      "9     9              1                         10                           1   \n",
      "\n",
      "   probability_rate  \n",
      "0               1.0  \n",
      "1               1.0  \n",
      "2               5.0  \n",
      "3               7.0  \n",
      "4               2.0  \n",
      "5               9.0  \n",
      "6              10.0  \n",
      "7               2.0  \n",
      "8               9.0  \n",
      "9              10.0  \n"
     ]
    }
   ],
   "source": [
    "print(test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e17cf8c-b583-46b5-9393-37e178c692e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент соответствия реального рейтинга с предсказанным мультиклассовой моделью: 42.272\n",
      "Процент соответствия реального рейтинга вероятности принадлежности классу \"positive\" 28.92\n"
     ]
    }
   ],
   "source": [
    "print('Процент соответствия реального рейтинга с предсказанным мультиклассовой моделью:', \n",
    "      (test['rate'] == test['predicted_rate_multiclass']).mean() * 100)\n",
    "print('Процент соответствия реального рейтинга вероятности принадлежности классу \"positive\"', \n",
    "      (test['rate'] == test['probability_rate']).mean() * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef1f866c-f20c-4e2b-a469-8caaf24ca257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент соответствия реальной окраски текста на соснове предсказаний мультиклассовой модели: 87.20400000000001\n",
      "Процент соответствия реальной окраски текста и пресказанной бинарной моделью: 88.472\n"
     ]
    }
   ],
   "source": [
    "print('Процент соответствия реальной окраски текста на соснове предсказаний мультиклассовой модели:', \n",
    "      (test['sentiment_bin'] == np.where(test['predicted_rate_multiclass'] > 5, 1, 0)).mean() * 100)\n",
    "print('Процент соответствия реальной окраски текста и пресказанной бинарной моделью:', \n",
    "      (test['sentiment_bin'] == test['predicted_semtiment_binary']).mean() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6b9b3-5328-46d7-9329-874fdd8266b6",
   "metadata": {},
   "source": [
    "Маленькая точность мультклассовой модели обусловлена многоклассовостью (классы от 1 до 10) и субъективностью оценок. По отзыву точно определить рейтинг фильма невозможно, так, например, негативный отзыв может иметь оценку 2, а модель может предсказать любое достаточно близкое значение (1 или 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931764d-6487-4674-81f8-77c585496687",
   "metadata": {},
   "source": [
    "## Экспорт модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea6515-7ee3-4e82-b354-e1f85d352711",
   "metadata": {},
   "source": [
    "Для создания сервиса импортируем модель мультиклассовой классификации, т.к. она имеет наиболее точные предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a831864-83e0-447a-ba29-a8160e1a31e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4626fc11-4c73-4cfc-8e88-4d0dbd43e534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_classification_model.pkl']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipe_text_multiclass_classfication, 'text_classification_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
